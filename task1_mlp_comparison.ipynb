{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9898019,"sourceType":"datasetVersion","datasetId":6079929}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:52:02.439231Z","iopub.execute_input":"2024-11-13T18:52:02.440314Z","iopub.status.idle":"2024-11-13T18:52:07.910272Z","shell.execute_reply.started":"2024-11-13T18:52:02.440252Z","shell.execute_reply":"2024-11-13T18:52:07.909248Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:52:07.914531Z","iopub.execute_input":"2024-11-13T18:52:07.915044Z","iopub.status.idle":"2024-11-13T18:52:07.923223Z","shell.execute_reply.started":"2024-11-13T18:52:07.914983Z","shell.execute_reply":"2024-11-13T18:52:07.922187Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\n\n# Define the transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Load the full dataset from the root directory containing subfolders for each class\ndataset = datasets.ImageFolder('/kaggle/input/assignment-4-dataset/images', transform=transform)\n\n# Split the dataset into 80/20 for train/test\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:52:07.924832Z","iopub.execute_input":"2024-11-13T18:52:07.925595Z","iopub.status.idle":"2024-11-13T18:52:08.013186Z","shell.execute_reply.started":"2024-11-13T18:52:07.925540Z","shell.execute_reply":"2024-11-13T18:52:08.012074Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class MLPModel(nn.Module):\n    def __init__(self):\n        super(MLPModel, self).__init__()\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(224 * 224 * 3, 4096)\n        self.fc2 = nn.Linear(4096, 2048)\n        self.fc3 = nn.Linear(2048, 1024)\n        self.fc4 = nn.Linear(1024, 512)\n        self.fc5 = nn.Linear(512, 2)  # Binary classification output\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc3(x))\n        x = self.relu(self.fc4(x))\n        x = self.fc5(x)\n        return x\n\nmlp_model = MLPModel().to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:52:08.015523Z","iopub.execute_input":"2024-11-13T18:52:08.015886Z","iopub.status.idle":"2024-11-13T18:52:14.486414Z","shell.execute_reply.started":"2024-11-13T18:52:08.015851Z","shell.execute_reply":"2024-11-13T18:52:14.485523Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from torchvision.models import VGG16_Weights, VGG19_Weights\n\ndef build_vgg_model(version='VGG16', tune_all=True):\n    if version == 'VGG16':\n        weights = VGG16_Weights.DEFAULT\n    else:\n        weights = VGG19_Weights.DEFAULT\n    \n    base_model = models.vgg16(weights=weights) if version == 'VGG16' else models.vgg19(weights=weights)\n\n    # Freeze convolution layers if not tuning all layers\n    if not tune_all:\n        for param in base_model.features.parameters():\n            param.requires_grad = False\n\n    # Modify the classifier for binary classification\n    base_model.classifier[6] = nn.Linear(base_model.classifier[6].in_features, 2)  # Binary classification output\n    return base_model.to(device)\n\n# Build VGG models with different tuning strategies\nvgg_tune_all = build_vgg_model(tune_all=True)\nvgg_tune_mlp = build_vgg_model(tune_all=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:52:14.487782Z","iopub.execute_input":"2024-11-13T18:52:14.488141Z","iopub.status.idle":"2024-11-13T18:52:20.772113Z","shell.execute_reply.started":"2024-11-13T18:52:14.488105Z","shell.execute_reply":"2024-11-13T18:52:20.771068Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 204MB/s]  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Step 4: Training Function\n\ndef train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * inputs.size(0)\n        epoch_loss = running_loss / len(dataloader.dataset)\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:52:20.774055Z","iopub.execute_input":"2024-11-13T18:52:20.774541Z","iopub.status.idle":"2024-11-13T18:52:20.782140Z","shell.execute_reply.started":"2024-11-13T18:52:20.774491Z","shell.execute_reply":"2024-11-13T18:52:20.781031Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Step 5: Training the Models\n\n# Set up loss and optimizers\ncriterion = nn.CrossEntropyLoss()\noptimizer_mlp = optim.Adam(mlp_model.parameters(), lr=0.0001)\noptimizer_vgg_all = optim.Adam(vgg_tune_all.parameters(), lr=0.0001)\noptimizer_vgg_mlp = optim.Adam(vgg_tune_mlp.parameters(), lr=0.0001)\n\n# Train each model\nprint(\"Training MLP Model...\")\ntrain_model(mlp_model, train_loader, criterion, optimizer_mlp)\n\nprint(\"Training VGG (Full Tuning) Model...\")\ntrain_model(vgg_tune_all, train_loader, criterion, optimizer_vgg_all)\n\nprint(\"Training VGG (MLP Tuning) Model...\")\ntrain_model(vgg_tune_mlp, train_loader, criterion, optimizer_vgg_mlp)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:52:20.783407Z","iopub.execute_input":"2024-11-13T18:52:20.784509Z","iopub.status.idle":"2024-11-13T18:59:10.019898Z","shell.execute_reply.started":"2024-11-13T18:52:20.784470Z","shell.execute_reply":"2024-11-13T18:59:10.018779Z"}},"outputs":[{"name":"stdout","text":"Training MLP Model...\nEpoch 1/10, Loss: 0.8542\nEpoch 2/10, Loss: 0.7091\nEpoch 3/10, Loss: 0.6548\nEpoch 4/10, Loss: 0.6242\nEpoch 5/10, Loss: 0.6281\nEpoch 6/10, Loss: 0.8784\nEpoch 7/10, Loss: 0.7276\nEpoch 8/10, Loss: 0.5470\nEpoch 9/10, Loss: 0.8032\nEpoch 10/10, Loss: 0.7040\nTraining VGG (Full Tuning) Model...\nEpoch 1/10, Loss: 0.2621\nEpoch 2/10, Loss: 0.0082\nEpoch 3/10, Loss: 0.0010\nEpoch 4/10, Loss: 0.0985\nEpoch 5/10, Loss: 0.1769\nEpoch 6/10, Loss: 0.4319\nEpoch 7/10, Loss: 0.0499\nEpoch 8/10, Loss: 0.0712\nEpoch 9/10, Loss: 0.0127\nEpoch 10/10, Loss: 0.0047\nTraining VGG (MLP Tuning) Model...\nEpoch 1/10, Loss: 0.3676\nEpoch 2/10, Loss: 0.0093\nEpoch 3/10, Loss: 0.0005\nEpoch 4/10, Loss: 0.0001\nEpoch 5/10, Loss: 0.0000\nEpoch 6/10, Loss: 0.0000\nEpoch 7/10, Loss: 0.0000\nEpoch 8/10, Loss: 0.0000\nEpoch 9/10, Loss: 0.0000\nEpoch 10/10, Loss: 0.0000\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Step 6: Evaluation Function\n\ndef evaluate_model(model, dataloader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    print(classification_report(all_labels, all_preds))\n    print(confusion_matrix(all_labels, all_preds))\n\n# Step 7: Evaluating the Models\n\nprint(\"MLP Model Evaluation:\")\nevaluate_model(mlp_model, test_loader)\n\nprint(\"VGG (Full Tuning) Model Evaluation:\")\nevaluate_model(vgg_tune_all, test_loader)\n\nprint(\"VGG (MLP Tuning) Model Evaluation:\")\nevaluate_model(vgg_tune_mlp, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:59:10.021552Z","iopub.execute_input":"2024-11-13T18:59:10.021995Z","iopub.status.idle":"2024-11-13T18:59:20.986069Z","shell.execute_reply.started":"2024-11-13T18:59:10.021942Z","shell.execute_reply":"2024-11-13T18:59:20.985049Z"}},"outputs":[{"name":"stdout","text":"MLP Model Evaluation:\n              precision    recall  f1-score   support\n\n           0       0.78      0.28      0.41        25\n           1       0.44      0.88      0.58        16\n\n    accuracy                           0.51        41\n   macro avg       0.61      0.58      0.50        41\nweighted avg       0.64      0.51      0.48        41\n\n[[ 7 18]\n [ 2 14]]\nVGG (Full Tuning) Model Evaluation:\n              precision    recall  f1-score   support\n\n           0       0.92      0.96      0.94        25\n           1       0.93      0.88      0.90        16\n\n    accuracy                           0.93        41\n   macro avg       0.93      0.92      0.92        41\nweighted avg       0.93      0.93      0.93        41\n\n[[24  1]\n [ 2 14]]\nVGG (MLP Tuning) Model Evaluation:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        25\n           1       1.00      1.00      1.00        16\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\n[[25  0]\n [ 0 16]]\n","output_type":"stream"}],"execution_count":8}]}